{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1UX2YR0ltXiYBUb911BWrYNoNYmWkW6kC",
      "authorship_tag": "ABX9TyMkliu9ngm4+NDdMVXU9lsl"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import metrics\n",
        "\n",
        "from joblib import parallel_backend\n",
        "import nltk"
      ],
      "metadata": {
        "id": "968QM6IvSKeP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATiTHdRJ0D_z",
        "outputId": "f6d97b4f-6707-4c5e-d3e6-cf587854b1de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Baixando os recursos necess√°rios do NLTK...\n",
            "‚úÖ Recursos baixados com sucesso!\n",
            "\n",
            "üîç Iniciando o programa...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dados carregados com sucesso.\n",
            "\n",
            "============================================================\n",
            "üìä Nomes das colunas de treinamento:\n",
            "Index(['id', 'entity', 'sentiment', 'text'], dtype='object')\n",
            "\n",
            "============================================================\n",
            "üìÑ Amostra dos dados de treinamento:\n",
            "     id       entity sentiment  \\\n",
            "0  2401  Borderlands  Positive   \n",
            "1  2401  Borderlands  Positive   \n",
            "2  2401  Borderlands  Positive   \n",
            "3  2401  Borderlands  Positive   \n",
            "4  2401  Borderlands  Positive   \n",
            "\n",
            "                                                text  \n",
            "0  im getting on borderlands and i will murder yo...  \n",
            "1  I am coming to the borders and I will kill you...  \n",
            "2  im getting on borderlands and i will kill you ...  \n",
            "3  im coming on borderlands and i will murder you...  \n",
            "4  im getting on borderlands 2 and i will murder ...  \n",
            "\n",
            "============================================================\n",
            "üìà Distribui√ß√£o das classes de sentimento de treinamento:\n",
            "sentiment\n",
            "Negative      22542\n",
            "Positive      20832\n",
            "Neutral       18318\n",
            "Irrelevant    12990\n",
            "Name: count, dtype: int64\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "============================================================\n",
            "üîß Melhores Par√¢metros: {'clf__alpha': 0.3, 'tfidf__max_features': 25000, 'tfidf__ngram_range': (1, 2)}\n",
            "\n",
            "============================================================\n",
            "üîç Pontua√ß√µes da Valida√ß√£o Cruzada:\n",
            "[0.80925495 0.80443795 0.80657006 0.80789276 0.80437873 0.80795199\n",
            " 0.80700057 0.80672418 0.80694135 0.80781   ]\n",
            "üî¢ Pontua√ß√£o m√©dia da Valida√ß√£o Cruzada: 0.8069\n",
            "‚úÖ Dados carregados com sucesso.\n",
            "\n",
            "============================================================\n",
            "üìà Relat√≥rio de Classifica√ß√£o no conjunto de valida√ß√£o:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Irrelevant       0.91      0.85      0.88       172\n",
            "    Negative       0.82      0.91      0.86       266\n",
            "     Neutral       0.92      0.80      0.85       285\n",
            "    Positive       0.87      0.92      0.90       277\n",
            "\n",
            "    accuracy                           0.87      1000\n",
            "   macro avg       0.88      0.87      0.87      1000\n",
            "weighted avg       0.88      0.87      0.87      1000\n",
            "\n",
            "\n",
            "============================================================\n",
            "üìä Matriz de Confus√£o no conjunto de valida√ß√£o:\n",
            "[[146  12   4  10]\n",
            " [  1 243  10  12]\n",
            " [  9  32 227  17]\n",
            " [  5  11   5 256]]\n",
            "\n",
            "============================================================\n",
            "üìù Testes com frases de exemplo:\n",
            "Frase: \"I love this product!\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"This is the worst experience I've ever had.\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"It's okay, not bad.\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"Absolutely fantastic!\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"I hate it so much.\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"The game is a bit boring.\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"I'm thrilled about this new update!\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"The movie was quite mediocre.\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"This is the best purchase I've ever made.\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"I'm really disappointed with the service.\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"The book was an interesting read.\"\n",
            "Sentimento Predito: Neutral\n",
            "\n",
            "Frase: \"I feel neutral about this feature.\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"This restaurant has excellent food.\"\n",
            "Sentimento Predito: Neutral\n",
            "\n",
            "Frase: \"The app crashes frequently, very frustrating.\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"I'm excited about the new release!\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"The product arrived late and damaged.\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"I enjoyed the concert a lot.\"\n",
            "Sentimento Predito: Irrelevant\n",
            "\n",
            "Frase: \"The experience was quite underwhelming.\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"I'm happy with my new phone.\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"The software update was a huge improvement.\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"I'm not satisfied with the customer support.\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"The quality of the product exceeded my expectations.\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"The weather was terrible during my vacation.\"\n",
            "Sentimento Predito: Neutral\n",
            "\n",
            "Frase: \"I had an amazing time at the event.\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"The service was slow but the food was good.\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"I'm not impressed with the new design.\"\n",
            "Sentimento Predito: Neutral\n",
            "\n",
            "Frase: \"The movie was entertaining and engaging.\"\n",
            "Sentimento Predito: Neutral\n",
            "\n",
            "Frase: \"I felt let down by the recent changes.\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"The trip was okay, nothing special.\"\n",
            "Sentimento Predito: Neutral\n",
            "\n",
            "Frase: \"I love the new features in the latest version.\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"The product is okay but could be better.\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"The restaurant ambiance was lovely.\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"I'm frustrated with the frequent bugs.\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"The book was a great read, highly recommended!\"\n",
            "Sentimento Predito: Irrelevant\n",
            "\n",
            "Frase: \"The service was excellent and prompt.\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"The new update made everything worse.\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"I feel indifferent about the new changes.\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"The performance was outstanding.\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"The quality did not meet my expectations.\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"I am very pleased with the purchase.\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"The user interface is much improved now.\"\n",
            "Sentimento Predito: Neutral\n",
            "\n",
            "Frase: \"The concert was an unforgettable experience.\"\n",
            "Sentimento Predito: Irrelevant\n",
            "\n",
            "Frase: \"The game is too repetitive and boring.\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"I am thrilled with the customer service!\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"The hotel was clean but the location was poor.\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"I'm dissatisfied with the recent upgrade.\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"The movie was a complete waste of time.\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"The new product features are amazing!\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "\n",
            "============================================================\n",
            "üîç An√°lise Cr√≠tica do Modelo:\n",
            "Pontos fortes:\n",
            "- O modelo utiliza t√©cnicas de pr√©-processamento e aumento de dados para melhorar o desempenho.\n",
            "- A implementa√ß√£o de Grid Search permite otimiza√ß√£o de hiperpar√¢metros.\n",
            "- O uso de valida√ß√£o cruzada fornece uma avalia√ß√£o mais robusta do modelo.\n",
            "\n",
            "Pontos fracos:\n",
            "- A detec√ß√£o de sentimentos sutis ou amb√≠guos pode ainda ser um desafio.\n",
            "- O modelo atual pode ter dificuldades com sarcasmo ou contextos mais complexos.\n",
            "- A depend√™ncia de um conjunto de dados espec√≠fico pode limitar a generaliza√ß√£o.\n",
            "\n",
            "Oportunidades de melhorias:\n",
            "- Explorar modelos mais avan√ßados, como redes neurais ou transformers.\n",
            "- Implementar t√©cnicas de an√°lise de erros para identificar padr√µes de falhas do modelo.\n",
            "- Considerar a inclus√£o de an√°lise de contexto ou informa√ß√µes adicionais para melhorar a precis√£o.\n",
            "- Expandir o conjunto de dados de treinamento com mais exemplos variados e complexos.\n",
            "\n",
            "============================================================\n",
            "‚ÑπÔ∏è Informa√ß√µes adicionais:\n",
            "Dados e pr√©-processamento:\n",
            "- Os dados foram aumentados usando t√©cnicas de aumento de dados.\n",
            "- O pr√©-processamento incluiu remo√ß√£o de URLs, n√∫meros e pontua√ß√µes.\n",
            "- Stopwords foram removidas, exceto palavras importantes para an√°lise de sentimento.\n",
            "- Todos os textos foram convertidos para min√∫sculas para padroniza√ß√£o.\n",
            "\n",
            "Modelo e treinamento:\n",
            "- Utilizou-se um pipeline com TfidfVectorizer e MultinomialNB.\n",
            "- O Grid Search foi aplicado para otimiza√ß√£o de hiperpar√¢metros.\n",
            "- A valida√ß√£o cruzada com 20 folds foi usada para avaliar o desempenho do modelo.\n",
            "\n",
            "Avalia√ß√£o e teste:\n",
            "- Um conjunto de valida√ß√£o separado foi usado para testar o modelo final.\n",
            "- Foram realizados testes com frases de exemplo para verificar o desempenho em casos variados.\n",
            "\n",
            "Limita√ß√µes e escopo:\n",
            "- O modelo atual suporta apenas an√°lises em ingl√™s.\n",
            "- O desempenho pode variar dependendo do contexto e complexidade das frases.\n",
            "- Sentimentos sutis ou amb√≠guos podem apresentar desafios para o modelo.\n"
          ]
        }
      ],
      "source": [
        "# Baixando os recursos necess√°rios do NLTK\n",
        "print(\"üîÑ Baixando os recursos necess√°rios do NLTK...\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "print(\"‚úÖ Recursos baixados com sucesso!\\n\")\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Pr√©-processa o texto de entrada realizando v√°rias opera√ß√µes de limpeza.\n",
        "\n",
        "    Args:\n",
        "    text (str): O texto de entrada a ser pr√©-processado.\n",
        "\n",
        "    Returns:\n",
        "    str: O texto pr√©-processado.\n",
        "    \"\"\"\n",
        "    if pd.isna(text):\n",
        "        return ''\n",
        "    if isinstance(text, str):\n",
        "        # Converter para min√∫sculas\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remover URLs\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "        # Remover n√∫meros e pontua√ß√µes, exceto '!' e '?'\n",
        "        text = re.sub(r'[^\\w\\s!?]', '', text)\n",
        "\n",
        "        # Remover espa√ßos em branco extras\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        # Tokeniza√ß√£o\n",
        "        tokens = word_tokenize(text.lower())\n",
        "\n",
        "        # Definir palavras de parada a serem mantidas (incluindo palavras relacionadas a emo√ß√µes)\n",
        "        stop_words = set(stopwords.words('english')) - {\n",
        "            'not', 'no', 'nor', 'but', 'however', 'okay', 'although', 'though',\n",
        "            'happy', 'sad', 'angry', 'frustrated', 'excited', 'disappointed',\n",
        "            # ... (resto das palavras relacionadas a emo√ß√µes)\n",
        "        }\n",
        "\n",
        "        # Filtrar tokens\n",
        "        filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "        return ' '.join(filtered_tokens)\n",
        "    return ''\n",
        "\n",
        "\n",
        "def get_synonyms(word):\n",
        "    \"\"\"\n",
        "    Obt√©m sin√¥nimos para uma palavra dada usando o WordNet.\n",
        "\n",
        "    Args:\n",
        "    word (str): A palavra de entrada para encontrar sin√¥nimos.\n",
        "\n",
        "    Returns:\n",
        "    list: Uma lista de sin√¥nimos para a palavra de entrada.\n",
        "    \"\"\"\n",
        "    return list({lemma.name() for syn in wordnet.synsets(word) for lemma in syn.lemmas()})\n",
        "\n",
        "\n",
        "def data_augmentation(text, sentiment, num_augmented=3, substitution_prob=0.3):\n",
        "    \"\"\"\n",
        "    Realiza aumento de dados no texto de entrada por substitui√ß√£o de sin√¥nimos e embaralhamento de palavras.\n",
        "\n",
        "    Args:\n",
        "    text (str): O texto de entrada para aumentar.\n",
        "    sentiment (str): O r√≥tulo de sentimento do texto de entrada.\n",
        "    num_augmented (int): N√∫mero de amostras aumentadas a serem geradas.\n",
        "    substitution_prob (float): Probabilidade de substituir uma palavra por seu sin√¥nimo.\n",
        "\n",
        "    Returns:\n",
        "    list: Uma lista de tuplas contendo textos aumentados e seus sentimentos.\n",
        "    \"\"\"\n",
        "    words = word_tokenize(text)\n",
        "    augmented_texts = []\n",
        "\n",
        "    # Substitui√ß√£o por sin√¥nimos\n",
        "    for _ in range(num_augmented):\n",
        "        new_text = []\n",
        "        for word in words:\n",
        "            synonyms = get_synonyms(word)\n",
        "            if synonyms and random.random() < substitution_prob:\n",
        "                new_text.append(random.choice(synonyms))\n",
        "            else:\n",
        "                new_text.append(word)\n",
        "        augmented_texts.append((' '.join(new_text), sentiment))\n",
        "\n",
        "    # Embaralhamento da ordem das palavras\n",
        "    for _ in range(num_augmented):\n",
        "        new_words = words.copy()\n",
        "        random.shuffle(new_words)\n",
        "        augmented_texts.append((' '.join(new_words), sentiment))\n",
        "\n",
        "    return augmented_texts\n",
        "\n",
        "\n",
        "def load_data(filepath):\n",
        "    \"\"\"\n",
        "    Carrega dados de um arquivo CSV.\n",
        "\n",
        "    Args:\n",
        "    filepath (str): O caminho para o arquivo CSV.\n",
        "\n",
        "    Returns:\n",
        "    pandas.DataFrame ou None: Os dados carregados como um DataFrame, ou None se ocorrer um erro.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        column_names = ['id', 'entity', 'sentiment', 'text']\n",
        "        df = pd.read_csv(filepath, header=None, names=column_names)\n",
        "        print(\"‚úÖ Dados carregados com sucesso.\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro ao carregar o dataset: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def print_divider():\n",
        "    \"\"\"Imprime uma linha divis√≥ria para melhor legibilidade da sa√≠da.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Fun√ß√£o principal para executar o pipeline de an√°lise de sentimentos.\n",
        "    \"\"\"\n",
        "    print(\"üîç Iniciando o programa...\")\n",
        "\n",
        "    # Carregando os dados de treinamento\n",
        "    df_train = load_data(\"/content/drive/MyDrive/Trabalho IA /twitter_training.csv\")\n",
        "\n",
        "    if df_train is not None:\n",
        "        print_divider()\n",
        "\n",
        "        # Explorando os dados de treinamento\n",
        "        print(\"üìä Nomes das colunas de treinamento:\")\n",
        "        print(df_train.columns)\n",
        "        print_divider()\n",
        "        print(\"üìÑ Amostra dos dados de treinamento:\")\n",
        "        print(df_train.head())\n",
        "\n",
        "        if 'sentiment' in df_train.columns:\n",
        "            print_divider()\n",
        "            print(\"üìà Distribui√ß√£o das classes de sentimento de treinamento:\")\n",
        "            print(df_train['sentiment'].value_counts())\n",
        "        else:\n",
        "            print(\"‚ùå Erro: A coluna 'sentiment' n√£o foi encontrada no DataFrame de treinamento.\")\n",
        "            return\n",
        "\n",
        "        # Remover linhas com valores NaN\n",
        "        df_train = df_train.dropna(subset=['text', 'sentiment'])\n",
        "\n",
        "        # Aplicar aumento de dados\n",
        "        augmented_data = []\n",
        "        for _, row in df_train.iterrows():\n",
        "            augmented_data.extend(data_augmentation(row['text'], row['sentiment'], num_augmented=3, substitution_prob=0.3))\n",
        "\n",
        "        # Adicionar dados aumentados ao DataFrame original\n",
        "        df_augmented = pd.DataFrame(augmented_data, columns=['text', 'sentiment'])\n",
        "        df_train = pd.concat([df_train, df_augmented], ignore_index=True)\n",
        "\n",
        "        # Embaralhar o DataFrame\n",
        "        df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "        # Pr√©-processamento dos dados de treinamento\n",
        "        df_train['clean_text'] = df_train['text'].apply(preprocess_text)\n",
        "\n",
        "        # Remover linhas vazias ap√≥s o pr√©-processamento\n",
        "        df_train = df_train[df_train['clean_text'] != '']\n",
        "\n",
        "        # Separa√ß√£o dos dados em caracter√≠sticas e alvo\n",
        "        X_train = df_train['clean_text']\n",
        "        y_train = df_train['sentiment']\n",
        "\n",
        "        # Criando o pipeline do modelo de PLN usando Naive Bayes\n",
        "        model = Pipeline([\n",
        "            ('tfidf', TfidfVectorizer()),\n",
        "            ('clf', MultinomialNB())\n",
        "        ])\n",
        "\n",
        "        # Definindo o grid de par√¢metros para ajuste de hiperpar√¢metros\n",
        "        parameters = {\n",
        "            'tfidf__ngram_range': [(1,2)],\n",
        "            'tfidf__max_features': [25000],\n",
        "            'clf__alpha': [0.3],\n",
        "        }\n",
        "\n",
        "        # Realizando Grid Search para ajuste de hiperpar√¢metros\n",
        "        grid_search = GridSearchCV(model, parameters, cv=10, n_jobs=-1, verbose=1)\n",
        "\n",
        "        # Usando parallel_backend para multi-threading\n",
        "        with parallel_backend('threading'):\n",
        "            grid_search.fit(X_train, y_train)\n",
        "\n",
        "        # Obtendo o melhor modelo e par√¢metros\n",
        "        best_model = grid_search.best_estimator_\n",
        "        print_divider()\n",
        "        print(f\"üîß Melhores Par√¢metros: {grid_search.best_params_}\")\n",
        "\n",
        "        # Realizando valida√ß√£o cruzada com o melhor modelo\n",
        "        cv_scores = cross_val_score(best_model, X_train, y_train, cv=10)\n",
        "        print_divider()\n",
        "        print(\"üîç Pontua√ß√µes da Valida√ß√£o Cruzada:\")\n",
        "        print(cv_scores)\n",
        "        print(f\"üî¢ Pontua√ß√£o m√©dia da Valida√ß√£o Cruzada: {np.mean(cv_scores):.4f}\")\n",
        "\n",
        "        # Carregando os dados de valida√ß√£o\n",
        "        df_val = load_data(\"/content/drive/MyDrive/Trabalho IA /twitter_validation.csv\")\n",
        "\n",
        "        if df_val is not None:\n",
        "            # Pr√©-processamento dos dados de valida√ß√£o\n",
        "            df_val['text'] = df_val['text'].fillna('').astype(str)\n",
        "            df_val['clean_text'] = df_val['text'].apply(preprocess_text)\n",
        "\n",
        "            # Separa√ß√£o dos dados de valida√ß√£o\n",
        "            X_val = df_val['clean_text']\n",
        "            y_val = df_val['sentiment']\n",
        "\n",
        "            # Avalia√ß√£o do modelo com o conjunto de valida√ß√£o\n",
        "            y_pred_val = best_model.predict(X_val)\n",
        "            print_divider()\n",
        "            print(\"üìà Relat√≥rio de Classifica√ß√£o no conjunto de valida√ß√£o:\")\n",
        "            print(metrics.classification_report(y_val, y_pred_val))\n",
        "\n",
        "            # Matriz de confus√£o no conjunto de valida√ß√£o\n",
        "            conf_matrix_val = metrics.confusion_matrix(y_val, y_pred_val)\n",
        "            print_divider()\n",
        "            print(\"üìä Matriz de Confus√£o no conjunto de valida√ß√£o:\")\n",
        "            print(conf_matrix_val)\n",
        "\n",
        "        # Realizando testes com frases de exemplo\n",
        "        exemplo_frases = [\n",
        "            \"I love this product!\",\n",
        "            \"This is the worst experience I've ever had.\",\n",
        "            \"It's okay, not bad.\",\n",
        "            \"Absolutely fantastic!\",\n",
        "            \"I hate it so much.\",\n",
        "            \"The game is a bit boring.\",\n",
        "            \"I'm thrilled about this new update!\",\n",
        "            \"The movie was quite mediocre.\",\n",
        "            \"This is the best purchase I've ever made.\",\n",
        "            \"I'm really disappointed with the service.\",\n",
        "            \"The book was an interesting read.\",\n",
        "            \"I feel neutral about this feature.\",\n",
        "            \"This restaurant has excellent food.\",\n",
        "            \"The app crashes frequently, very frustrating.\",\n",
        "            \"I'm excited about the new release!\",\n",
        "            \"The product arrived late and damaged.\",\n",
        "            \"I enjoyed the concert a lot.\",\n",
        "            \"The experience was quite underwhelming.\",\n",
        "            \"I'm happy with my new phone.\",\n",
        "            \"The software update was a huge improvement.\",\n",
        "            \"I'm not satisfied with the customer support.\",\n",
        "            \"The quality of the product exceeded my expectations.\",\n",
        "            \"The weather was terrible during my vacation.\",\n",
        "            \"I had an amazing time at the event.\",\n",
        "            \"The service was slow but the food was good.\",\n",
        "            \"I'm not impressed with the new design.\",\n",
        "            \"The movie was entertaining and engaging.\",\n",
        "            \"I felt let down by the recent changes.\",\n",
        "            \"The trip was okay, nothing special.\",\n",
        "            \"I love the new features in the latest version.\",\n",
        "            \"The product is okay but could be better.\",\n",
        "            \"The restaurant ambiance was lovely.\",\n",
        "            \"I'm frustrated with the frequent bugs.\",\n",
        "            \"The book was a great read, highly recommended!\",\n",
        "            \"The service was excellent and prompt.\",\n",
        "            \"The new update made everything worse.\",\n",
        "            \"I feel indifferent about the new changes.\",\n",
        "            \"The performance was outstanding.\",\n",
        "            \"The quality did not meet my expectations.\",\n",
        "            \"I am very pleased with the purchase.\",\n",
        "            \"The user interface is much improved now.\",\n",
        "            \"The concert was an unforgettable experience.\",\n",
        "            \"The game is too repetitive and boring.\",\n",
        "            \"I am thrilled with the customer service!\",\n",
        "            \"The hotel was clean but the location was poor.\",\n",
        "            \"I'm dissatisfied with the recent upgrade.\",\n",
        "            \"The movie was a complete waste of time.\",\n",
        "            \"The new product features are amazing!\"\n",
        "        ]\n",
        "\n",
        "        predicoes = best_model.predict(exemplo_frases)\n",
        "        print_divider()\n",
        "        print(\"üìù Testes com frases de exemplo:\")\n",
        "        for frase, sentimento in zip(exemplo_frases, predicoes):\n",
        "            print(f'Frase: \"{frase}\"\\nSentimento Predito: {sentimento}\\n')\n",
        "\n",
        "        # An√°lise cr√≠tica do modelo\n",
        "        print_divider()\n",
        "        print(\"üîç An√°lise Cr√≠tica do Modelo:\")\n",
        "        print(\"Pontos fortes:\")\n",
        "        print(\"- O modelo utiliza t√©cnicas de pr√©-processamento e aumento de dados para melhorar o desempenho.\")\n",
        "        print(\"- A implementa√ß√£o de Grid Search permite otimiza√ß√£o de hiperpar√¢metros.\")\n",
        "        print(\"- O uso de valida√ß√£o cruzada fornece uma avalia√ß√£o mais robusta do modelo.\")\n",
        "        print(\"\\nPontos fracos:\")\n",
        "        print(\"- A detec√ß√£o de sentimentos sutis ou amb√≠guos pode ainda ser um desafio.\")\n",
        "        print(\"- O modelo atual pode ter dificuldades com sarcasmo ou contextos mais complexos.\")\n",
        "        print(\"- A depend√™ncia de um conjunto de dados espec√≠fico pode limitar a generaliza√ß√£o.\")\n",
        "        print(\"\\nOportunidades de melhorias:\")\n",
        "        print(\"- Explorar modelos mais avan√ßados, como redes neurais ou transformers.\")\n",
        "        print(\"- Implementar t√©cnicas de an√°lise de erros para identificar padr√µes de falhas do modelo.\")\n",
        "        print(\"- Considerar a inclus√£o de an√°lise de contexto ou informa√ß√µes adicionais para melhorar a precis√£o.\")\n",
        "        print(\"- Expandir o conjunto de dados de treinamento com mais exemplos variados e complexos.\")\n",
        "\n",
        "        # Informa√ß√µes adicionais\n",
        "        print_divider()\n",
        "        print(\"‚ÑπÔ∏è Informa√ß√µes adicionais:\")\n",
        "        print(\"Dados e pr√©-processamento:\")\n",
        "        print(\"- Os dados foram aumentados usando t√©cnicas de aumento de dados.\")\n",
        "        print(\"- O pr√©-processamento incluiu remo√ß√£o de URLs, n√∫meros e pontua√ß√µes.\")\n",
        "        print(\"- Stopwords foram removidas, exceto palavras importantes para an√°lise de sentimento.\")\n",
        "        print(\"- Todos os textos foram convertidos para min√∫sculas para padroniza√ß√£o.\")\n",
        "\n",
        "        print(\"\\nModelo e treinamento:\")\n",
        "        print(\"- Utilizou-se um pipeline com TfidfVectorizer e MultinomialNB.\")\n",
        "        print(\"- O Grid Search foi aplicado para otimiza√ß√£o de hiperpar√¢metros.\")\n",
        "        print(\"- A valida√ß√£o cruzada com 20 folds foi usada para avaliar o desempenho do modelo.\")\n",
        "\n",
        "        print(\"\\nAvalia√ß√£o e teste:\")\n",
        "        print(\"- Um conjunto de valida√ß√£o separado foi usado para testar o modelo final.\")\n",
        "        print(\"- Foram realizados testes com frases de exemplo para verificar o desempenho em casos variados.\")\n",
        "\n",
        "        print(\"\\nLimita√ß√µes e escopo:\")\n",
        "        print(\"- O modelo atual suporta apenas an√°lises em ingl√™s.\")\n",
        "        print(\"- O desempenho pode variar dependendo do contexto e complexidade das frases.\")\n",
        "        print(\"- Sentimentos sutis ou amb√≠guos podem apresentar desafios para o modelo.\")\n",
        "    else:\n",
        "        print(\"‚ùå N√£o foi poss√≠vel carregar os dados. Verifique o caminho do arquivo e tente novamente.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}