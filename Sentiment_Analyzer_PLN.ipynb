{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1UX2YR0ltXiYBUb911BWrYNoNYmWkW6kC",
      "authorship_tag": "ABX9TyN/+40RB1IGDb/nwa4AZxN8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import metrics\n",
        "import re\n",
        "from joblib import parallel_backend\n",
        "import random\n",
        "\n",
        "\n",
        "!pip install textblob\n",
        "!pip install pyspellchecker\n",
        "from spellchecker import SpellChecker\n",
        "# Implementar corre√ß√µes ortogr√°ficas\n",
        "from textblob import TextBlob\n"
      ],
      "metadata": {
        "id": "968QM6IvSKeP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c744894-b436-4b5e-a04e-853eab99573a"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.5)\n",
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.10/dist-packages (0.8.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Baixando os recursos necess√°rios do NLTK\n",
        "print(\"üîÑ Baixando os recursos necess√°rios do NLTK...\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "print(\"‚úÖ Recursos baixados com sucesso!!\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YOW0c0wXk9H",
        "outputId": "bdbd5ff8-313a-41ce-f2e8-edcb4dae7f0d"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Baixando os recursos necess√°rios do NLTK...\n",
            "‚úÖ Recursos baixados com sucesso!!\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATiTHdRJ0D_z",
        "outputId": "3d4cd941-7466-446c-b604-a47340fd8b59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Iniciando o programa...\n",
            "‚úÖ Dados carregados com sucesso.\n",
            "\n",
            "============================================================\n",
            "üìä Nomes das colunas de treinamento:\n",
            "Index(['id', 'entity', 'sentiment', 'text'], dtype='object')\n",
            "\n",
            "============================================================\n",
            "üìÑ Amostra dos dados de treinamento:\n",
            "     id       entity sentiment  \\\n",
            "0  2401  Borderlands  Positive   \n",
            "1  2401  Borderlands  Positive   \n",
            "2  2401  Borderlands  Positive   \n",
            "3  2401  Borderlands  Positive   \n",
            "4  2401  Borderlands  Positive   \n",
            "\n",
            "                                                text  \n",
            "0  im getting on borderlands and i will murder yo...  \n",
            "1  I am coming to the borders and I will kill you...  \n",
            "2  im getting on borderlands and i will kill you ...  \n",
            "3  im coming on borderlands and i will murder you...  \n",
            "4  im getting on borderlands 2 and i will murder ...  \n",
            "\n",
            "============================================================\n",
            "üìà Distribui√ß√£o das classes de sentimento de treinamento:\n",
            "sentiment\n",
            "Negative      22542\n",
            "Positive      20832\n",
            "Neutral       18318\n",
            "Irrelevant    12990\n",
            "Name: count, dtype: int64\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "\n",
            "============================================================\n",
            "üîß Melhores Par√¢metros: {'clf__alpha': 0.3, 'tfidf__max_features': 20000, 'tfidf__ngram_range': (1, 2)}\n",
            "\n",
            "============================================================\n",
            "üîç Pontua√ß√µes da Verifica√ß√£o Cruzada:\n",
            "[0.76742901 0.76715263 0.76735991 0.76763629 0.7662544  0.76817302\n",
            " 0.7699696  0.76250691 0.7699005  0.76485627]\n",
            "üî¢ Pontua√ß√£o m√©dia da Verifica√ß√£o Cruzada: 0.7671\n",
            "‚úÖ Dados carregados com sucesso.\n",
            "\n",
            "============================================================\n",
            "üìà Relat√≥rio de Classifica√ß√£o no conjunto de valida√ß√£o:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Irrelevant       0.89      0.79      0.84       172\n",
            "    Negative       0.80      0.91      0.85       266\n",
            "     Neutral       0.89      0.78      0.83       285\n",
            "    Positive       0.86      0.92      0.89       277\n",
            "\n",
            "    accuracy                           0.85      1000\n",
            "   macro avg       0.86      0.85      0.85      1000\n",
            "weighted avg       0.86      0.85      0.85      1000\n",
            "\n",
            "\n",
            "============================================================\n",
            "üìä Matriz de Confus√£o no conjunto de valida√ß√£o:\n",
            "[[136  15   8  13]\n",
            " [  1 241  12  12]\n",
            " [ 11  36 221  17]\n",
            " [  5  10   8 254]]\n",
            "\n",
            "============================================================\n",
            "üìù Testes com frases de exemplo:\n",
            "Frase: \"I love this product!\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"This is the worst experience I've ever had.\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"It's okay, not bad.\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"Absolutely fantastic!\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"I hate it so much.\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"The game is a bit boring.\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"I'm thrilled about this new update!\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"The movie was quite mediocre.\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"This is the best purchase I've ever made.\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"I'm really disappointed with the service.\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"The book was an interesting read.\"\n",
            "Sentimento Predito: Neutral\n",
            "\n",
            "Frase: \"I feel neutral about this feature.\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"This restaurant has excellent food.\"\n",
            "Sentimento Predito: Neutral\n",
            "\n",
            "Frase: \"The app crashes frequently, very frustrating.\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"I'm excited about the new release!\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"The product arrived late and damaged.\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"I enjoyed the concert a lot.\"\n",
            "Sentimento Predito: Irrelevant\n",
            "\n",
            "Frase: \"The experience was quite underwhelming.\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"I'm happy with my new phone.\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"The software update was a huge improvement.\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"I'm not satisfied with the customer support.\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"The quality of the product exceeded my expectations.\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"The weather was terrible during my vacation.\"\n",
            "Sentimento Predito: Neutral\n",
            "\n",
            "Frase: \"I had an amazing time at the event.\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"The service was slow but the food was good.\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"I'm not impressed with the new design.\"\n",
            "Sentimento Predito: Neutral\n",
            "\n",
            "Frase: \"The movie was entertaining and engaging.\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"I felt let down by the recent changes.\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"The trip was okay, nothing special.\"\n",
            "Sentimento Predito: Neutral\n",
            "\n",
            "Frase: \"I love the new features in the latest version.\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"The product is okay but could be better.\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"The restaurant ambiance was lovely.\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"I'm frustrated with the frequent bugs.\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"The book was a great read, highly recommended!\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"The service was excellent and prompt.\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"The new update made everything worse.\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"I feel indifferent about the new changes.\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"The performance was outstanding.\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"The quality did not meet my expectations.\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"I am very pleased with the purchase.\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"The user interface is much improved now.\"\n",
            "Sentimento Predito: Neutral\n",
            "\n",
            "Frase: \"The concert was an unforgettable experience.\"\n",
            "Sentimento Predito: Irrelevant\n",
            "\n",
            "Frase: \"The game is too repetitive and boring.\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"I am thrilled with the customer service!\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"The hotel was clean but the location was poor.\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "Frase: \"I'm dissatisfied with the recent upgrade.\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"The movie was a complete waste of time.\"\n",
            "Sentimento Predito: Negative\n",
            "\n",
            "Frase: \"The new product features are amazing!\"\n",
            "Sentimento Predito: Positive\n",
            "\n",
            "\n",
            "============================================================\n",
            "üîç An√°lise Cr√≠tica do Modelo:\n",
            "Pontos fortes: O modelo ajustado pode apresentar melhorias em desempenho.\n",
            "Pontos fracos: A detec√ß√£o de sentimentos sutis ainda pode ser um desafio.\n",
            "Oportunidades de melhorias: Continuar ajustando hiperpar√¢metros e explorar modelos mais avan√ßados.\n",
            "\n",
            "============================================================\n",
            "‚ÑπÔ∏è Informa√ß√µes adicionais:\n",
            "Os dados foram divididos em 80% para treino e 20% para teste.\n",
            "O pr√©-processamento incluiu a remo√ß√£o de stopwords e convers√£o para min√∫sculas.\n",
            "O modelo foi ajustado com Grid Search para melhorar o desempenho.\n",
            "O modelo suporta an√°lises em ingl√™s.\n"
          ]
        }
      ],
      "source": [
        "def preprocess_text(text):\n",
        "    if pd.isna(text):\n",
        "        return ''\n",
        "    if isinstance(text, str):\n",
        "        # Converter para min√∫sculas\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remover URLs\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "        # Remover n√∫meros e pontua√ß√µes, exceto '!' e '?'\n",
        "        text = re.sub(r'[^\\w\\s!?]', '', text)\n",
        "\n",
        "        # Remover espa√ßos em branco extras\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        # Tokeniza√ß√£o\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        stop_words = set(stopwords.words('english')) - {  'not', 'no', 'nor', 'but', 'however', 'okay' , 'although', 'though',\n",
        "                                                          'happy', 'sad', 'angry', 'frustrated', 'excited', 'disappointed',\n",
        "                                                          'anxious', 'calm', 'hopeful', 'fearful', 'joyful', 'worried',\n",
        "                                                          'content', 'lonely', 'confused', 'relieved', 'depressed',\n",
        "                                                          'satisfied', 'jealous', 'grateful', 'guilty', 'ashamed',\n",
        "                                                          'proud', 'embarrassed', 'nervous', 'surprised', 'curious',\n",
        "                                                          'bored', 'tired', 'energetic', 'peaceful', 'resentful',\n",
        "                                                          'inspired', 'motivated', 'determined', 'overwhelmed',\n",
        "                                                          'irritated', 'hurt', 'loving', 'caring', 'appreciated',\n",
        "                                                          'ignored', 'unimportant', 'valued', 'respected', 'admired',\n",
        "                                                          'misunderstood', 'rejected', 'accepted', 'supported'\n",
        "                                                       }\n",
        "        filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "        return ' '.join(filtered_tokens)\n",
        "    return ''\n",
        "\n",
        "def get_synonyms(word):\n",
        "    return list({lemma.name() for syn in wordnet.synsets(word) for lemma in syn.lemmas()})\n",
        "\n",
        "def data_augmentation(text, sentiment, num_augmented=1, substitution_prob=0.3):\n",
        "    words = text.split()\n",
        "    augmented_texts = []\n",
        "\n",
        "    for _ in range(num_augmented):\n",
        "        new_text = []\n",
        "        for word in words:\n",
        "            synonyms = get_synonyms(word)\n",
        "            if synonyms and random.random() < substitution_prob:\n",
        "                new_text.append(random.choice(synonyms))\n",
        "            else:\n",
        "                new_text.append(word)\n",
        "        augmented_texts.append((' '.join(new_text), sentiment))\n",
        "\n",
        "    return augmented_texts\n",
        "def load_data(filepath):\n",
        "    try:\n",
        "        column_names = ['id', 'entity', 'sentiment', 'text']\n",
        "        df = pd.read_csv(filepath, header=None, names=column_names)\n",
        "        print(\"‚úÖ Dados carregados com sucesso.\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro ao carregar o dataset: {e}\")\n",
        "        return None\n",
        "\n",
        "def print_divider():\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "def main():\n",
        "    print(\"üîç Iniciando o programa...\")\n",
        "\n",
        "    # Carregando os dados de treinamento\n",
        "    df_train = load_data(\"/content/drive/MyDrive/Trabalho IA /twitter_training.csv\")\n",
        "\n",
        "    if df_train is not None:\n",
        "        print_divider()\n",
        "\n",
        "        # Explorando os dados de treinamento\n",
        "        print(\"üìä Nomes das colunas de treinamento:\")\n",
        "        print(df_train.columns)\n",
        "        print_divider()\n",
        "        print(\"üìÑ Amostra dos dados de treinamento:\")\n",
        "        print(df_train.head())\n",
        "\n",
        "        if 'sentiment' in df_train.columns:\n",
        "            print_divider()\n",
        "            print(\"üìà Distribui√ß√£o das classes de sentimento de treinamento:\")\n",
        "            print(df_train['sentiment'].value_counts())\n",
        "        else:\n",
        "            print(\"‚ùå Erro: A coluna 'sentiment' n√£o foi encontrada no DataFrame de treinamento.\")\n",
        "            return\n",
        "\n",
        "        # Remover linhas com valores NaN\n",
        "        df_train = df_train.dropna(subset=['text', 'sentiment'])\n",
        "\n",
        "        # Aplicar data augmentation\n",
        "        augmented_data = []\n",
        "        for _, row in df_train.iterrows():\n",
        "            augmented_data.extend(data_augmentation(row['text'], row['sentiment'], num_augmented=1, substitution_prob=0.3))\n",
        "\n",
        "        # Adicionar dados aumentados ao DataFrame original\n",
        "        df_augmented = pd.DataFrame(augmented_data, columns=['text', 'sentiment'])\n",
        "        df_train = pd.concat([df_train, df_augmented], ignore_index=True)\n",
        "\n",
        "        # Embaralhar o DataFrame\n",
        "        df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "        # Pr√©-processamento dos dados de treinamento\n",
        "        df_train['clean_text'] = df_train['text'].apply(preprocess_text)\n",
        "\n",
        "        # Remover linhas vazias ap√≥s o pr√©-processamento\n",
        "        df_train = df_train[df_train['clean_text'] != '']\n",
        "\n",
        "        # Separa√ß√£o dos dados em treino e teste\n",
        "        X_train = df_train['clean_text']\n",
        "        y_train = df_train['sentiment']\n",
        "\n",
        "        # Criando o modelo de PLN, utilizando Naive Bayes\n",
        "        model = Pipeline([\n",
        "            ('tfidf', TfidfVectorizer()),\n",
        "            ('clf', MultinomialNB())\n",
        "        ])\n",
        "\n",
        "        # Definindo o grid de par√¢metros para ajuste\n",
        "        parameters = {\n",
        "            'tfidf__ngram_range': [(1,2)],\n",
        "            'tfidf__max_features': [20000],\n",
        "            'clf__alpha': [0.3]\n",
        "        }\n",
        "\n",
        "        # Grid Search para ajuste de hiperpar√¢metros\n",
        "        grid_search = GridSearchCV(model, parameters, cv=10, n_jobs=-1, verbose=1)\n",
        "\n",
        "        # Usando parallel_backend para definir o uso de threads\n",
        "        with parallel_backend('threading'):\n",
        "            grid_search.fit(X_train, y_train)\n",
        "\n",
        "        # Melhor modelo e par√¢metros\n",
        "        best_model = grid_search.best_estimator_\n",
        "        print_divider()\n",
        "        print(f\"üîß Melhores Par√¢metros: {grid_search.best_params_}\")\n",
        "\n",
        "        # Verifica√ß√£o cruzada com o melhor modelo\n",
        "        cv_scores = cross_val_score(best_model, X_train, y_train, cv=10)  # 5-fold cross-validation\n",
        "        print_divider()\n",
        "        print(\"üîç Pontua√ß√µes da Verifica√ß√£o Cruzada:\")\n",
        "        print(cv_scores)\n",
        "        print(f\"üî¢ Pontua√ß√£o m√©dia da Verifica√ß√£o Cruzada: {np.mean(cv_scores):.4f}\")\n",
        "\n",
        "        # Carregando os dados de valida√ß√£o\n",
        "        df_val = load_data(\"/content/drive/MyDrive/Trabalho IA /twitter_validation.csv\")\n",
        "\n",
        "        if df_val is not None:\n",
        "            # Pr√©-processamento dos dados de valida√ß√£o\n",
        "            df_val['text'] = df_val['text'].fillna('').astype(str)\n",
        "            df_val['clean_text'] = df_val['text'].apply(preprocess_text)\n",
        "\n",
        "            # Separa√ß√£o dos dados de valida√ß√£o\n",
        "            X_val = df_val['clean_text']\n",
        "            y_val = df_val['sentiment']\n",
        "\n",
        "            # Avalia√ß√£o do modelo com o conjunto de valida√ß√£o\n",
        "            y_pred_val = best_model.predict(X_val)\n",
        "            print_divider()\n",
        "            print(\"üìà Relat√≥rio de Classifica√ß√£o no conjunto de valida√ß√£o:\")\n",
        "            print(metrics.classification_report(y_val, y_pred_val))\n",
        "\n",
        "            # Matriz de confus√£o no conjunto de valida√ß√£o\n",
        "            conf_matrix_val = metrics.confusion_matrix(y_val, y_pred_val)\n",
        "            print_divider()\n",
        "            print(\"üìä Matriz de Confus√£o no conjunto de valida√ß√£o:\")\n",
        "            print(conf_matrix_val)\n",
        "\n",
        "        # Realizando testes com frases de exemplo\n",
        "        exemplo_frases = [\n",
        "            \"I love this product!\",\n",
        "            \"This is the worst experience I've ever had.\",\n",
        "            \"It's okay, not bad.\",\n",
        "            \"Absolutely fantastic!\",\n",
        "            \"I hate it so much.\",\n",
        "            \"The game is a bit boring.\",\n",
        "            \"I'm thrilled about this new update!\",\n",
        "            \"The movie was quite mediocre.\",\n",
        "            \"This is the best purchase I've ever made.\",\n",
        "            \"I'm really disappointed with the service.\",\n",
        "            \"The book was an interesting read.\",\n",
        "            \"I feel neutral about this feature.\",\n",
        "            \"This restaurant has excellent food.\",\n",
        "            \"The app crashes frequently, very frustrating.\",\n",
        "            \"I'm excited about the new release!\",\n",
        "            \"The product arrived late and damaged.\",\n",
        "            \"I enjoyed the concert a lot.\",\n",
        "            \"The experience was quite underwhelming.\",\n",
        "            \"I'm happy with my new phone.\",\n",
        "            \"The software update was a huge improvement.\",\n",
        "            \"I'm not satisfied with the customer support.\",\n",
        "            \"The quality of the product exceeded my expectations.\",\n",
        "            \"The weather was terrible during my vacation.\",\n",
        "            \"I had an amazing time at the event.\",\n",
        "            \"The service was slow but the food was good.\",\n",
        "            \"I'm not impressed with the new design.\",\n",
        "            \"The movie was entertaining and engaging.\",\n",
        "            \"I felt let down by the recent changes.\",\n",
        "            \"The trip was okay, nothing special.\",\n",
        "            \"I love the new features in the latest version.\",\n",
        "            \"The product is okay but could be better.\",\n",
        "            \"The restaurant ambiance was lovely.\",\n",
        "            \"I'm frustrated with the frequent bugs.\",\n",
        "            \"The book was a great read, highly recommended!\",\n",
        "            \"The service was excellent and prompt.\",\n",
        "            \"The new update made everything worse.\",\n",
        "            \"I feel indifferent about the new changes.\",\n",
        "            \"The performance was outstanding.\",\n",
        "            \"The quality did not meet my expectations.\",\n",
        "            \"I am very pleased with the purchase.\",\n",
        "            \"The user interface is much improved now.\",\n",
        "            \"The concert was an unforgettable experience.\",\n",
        "            \"The game is too repetitive and boring.\",\n",
        "            \"I am thrilled with the customer service!\",\n",
        "            \"The hotel was clean but the location was poor.\",\n",
        "            \"I'm dissatisfied with the recent upgrade.\",\n",
        "            \"The movie was a complete waste of time.\",\n",
        "            \"The new product features are amazing!\"\n",
        "        ]\n",
        "\n",
        "        predicoes = best_model.predict(exemplo_frases)\n",
        "        print_divider()\n",
        "        print(\"üìù Testes com frases de exemplo:\")\n",
        "        for frase, sentimento in zip(exemplo_frases, predicoes):\n",
        "            print(f'Frase: \"{frase}\"\\nSentimento Predito: {sentimento}\\n')\n",
        "\n",
        "        # An√°lise cr√≠tica do modelo\n",
        "        print_divider()\n",
        "        print(\"üîç An√°lise Cr√≠tica do Modelo:\")\n",
        "        print(\"Pontos fortes: O modelo ajustado pode apresentar melhorias em desempenho.\")\n",
        "        print(\"Pontos fracos: A detec√ß√£o de sentimentos sutis ainda pode ser um desafio.\")\n",
        "        print(\"Oportunidades de melhorias: Continuar ajustando hiperpar√¢metros e explorar modelos mais avan√ßados.\")\n",
        "\n",
        "        # Informa√ß√µes adicionais\n",
        "        print_divider()\n",
        "        print(\"‚ÑπÔ∏è Informa√ß√µes adicionais:\")\n",
        "        print(\"Os dados foram divididos em 80% para treino e 20% para teste.\")\n",
        "        print(\"O pr√©-processamento incluiu a remo√ß√£o de stopwords e convers√£o para min√∫sculas.\")\n",
        "        print(\"O modelo foi ajustado com Grid Search para melhorar o desempenho.\")\n",
        "        print(\"O modelo suporta an√°lises em ingl√™s.\")\n",
        "    else:\n",
        "        print(\"‚ùå N√£o foi poss√≠vel carregar os dados. Verifique o caminho do arquivo e tente novamente.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}